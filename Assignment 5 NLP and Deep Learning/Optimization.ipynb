{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f482f52-9410-4501-adce-5ed76e47d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk   \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from newspaper import Article\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "import _locale\n",
    "import warnings\n",
    "from numba import cuda \n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94be158-56e0-4d53-9ebf-785447c944c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    def text_process(mess):\n",
    "\n",
    "\n",
    "        stopwordList = stopwords.words('english')\n",
    "\n",
    "        mess = [char for char in mess if char not in string.punctuation]\n",
    "        mess = ''.join(mess)\n",
    "\n",
    "        words = nltk.word_tokenize(mess)\n",
    "        words = [t for t in words if t not in stopwordList]\n",
    "        words = [stemmer.stem(w.lower()) for w in words]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "        coo = X.tocoo()\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "    t0 = time.time()\n",
    "    \n",
    "    df = pd.read_csv(\"news.csv\", sep = \"\\t\")\n",
    "    df[\"target\"] = df[\"subject\"]\n",
    "    df[\"TitleText\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "    df = df.drop(columns = [\"title\", \"text\", \"subject\", \"date\"])\n",
    "    df.target = df[\"target\"].astype(\"category\").cat.codes\n",
    "    subject = tf.keras.utils.to_categorical(df[\"target\"].values, num_classes=8)\n",
    "    text_train, text_test, y_train, y_test = train_test_split(df[\"TitleText\"], subject, test_size=0.2, random_state=101)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Time to Load and Split Data: \" + str(t1-t0))\n",
    "    \n",
    "    bow_transformer = CountVectorizer(analyzer=text_process).fit(df[\"TitleText\"])\n",
    "    text_bow = bow_transformer.transform(df[\"TitleText\"])\n",
    "    tfidf_transformer = TfidfTransformer().fit(text_bow)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to Fit TFIDF Model: \" + str(t2-t1))\n",
    "    \n",
    "    train_bow = bow_transformer.transform(text_train)\n",
    "    train_tfidf = tfidf_transformer.transform(train_bow)\n",
    "    test_bow = bow_transformer.transform(text_test)\n",
    "    test_tfidf = tfidf_transformer.transform(test_bow)\n",
    "    \n",
    "    t3 = time.time()\n",
    "    print(\"Time to Vectorize train and test sets: \" + str(t3-t2))\n",
    "    \n",
    "    train_tensor = convert_sparse_matrix_to_sparse_tensor(train_tfidf)\n",
    "    x_train = tf.sparse.reorder(train_tensor)\n",
    "    test_tensor = convert_sparse_matrix_to_sparse_tensor(test_tfidf)\n",
    "    x_test = tf.sparse.reorder(test_tensor)\n",
    "    \n",
    "    t4 = time.time()\n",
    "    print(\"Time to Convert Matrix into Tensor and Reorder: \" + str(t4-t3))\n",
    "    print(\"Time to execute: \" + str(t4-t0))\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe9fbd2-ce17-4af1-aa5d-cf18135a7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, y_train, x_test, y_test):\n",
    "    model = tf.keras.Sequential()\n",
    "    size = {{choice([8,10,12,14,16,18,20,22,24,26,28,30,32])}}\n",
    "    activations={{choice(['relu', 'tanh'])}}\n",
    "    choiceval = {{choice(['adam', 'rmsprop'])}}\n",
    "    lr = {{uniform(0.0009, 0.00225)}}\n",
    "    adam = tf.keras.optimizers.Adam(lr=lr)\n",
    "    rmsprop = tf.keras.optimizers.RMSprop(lr=lr)\n",
    "    hidden = {{choice([2,3,4,5,6])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    model.add(layers.Dense(size, input_dim = x_train.shape[1], activation = activations)) # input layer requires input_dim param\n",
    "    for i in range(hidden-1):\n",
    "        model.add(layers.Dense(size, activation = activations))\n",
    "    model.add(layers.Dense(8, activation='softmax'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer= optim, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0025, patience=1, verbose=0, mode='auto')\n",
    "    with tf.device('/cpu:0'):\n",
    "        model.fit(x_train, y_train, epochs = 100, shuffle = True, batch_size=128, verbose=0, callbacks=[es])\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    loss = score[0]\n",
    "    return {'loss': loss, 'status': STATUS_OK, 'model': model} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89fde7-295a-4d48-9185-d5aba5d4ee7e",
   "metadata": {},
   "source": [
    "## Always Load Data with CPU (Just as Fast + Doesn't take All the Vram to Load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1059f638-7ea7-4c41-8550-55737e1a3ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to Load and Split Data: 1.0114963054656982\n",
      "Time to Fit TFIDF Model: 344.6874997615814\n",
      "Time to Vectorize train and test sets: 172.36899876594543\n",
      "Time to Convert Matrix into Tensor and Reorder: 0.5100018978118896\n",
      "Time to execute: 518.5779967308044\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    x_train, y_train, x_test, y_test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236458c-4134-40ec-b1ab-56defbe47644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               47362560  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 72        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,430,480\n",
      "Trainable params: 47,430,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 11s 38ms/step - loss: 1.4541 - acc: 0.5493 - val_loss: 1.0714 - val_acc: 0.7252\n",
      "Epoch 2/100\n",
      " 89/281 [========>.....................] - ETA: 6s - loss: 1.0036 - acc: 0.7616"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "m = tf.keras.Sequential()\n",
    "activations = \"tanh\"\n",
    "choiceval = \"rmsprop\"\n",
    "lr = 0.0001\n",
    "adam = tf.keras.optimizers.Adam(lr=lr)\n",
    "rmsprop = tf.keras.optimizers.RMSprop(lr=lr)\n",
    "hidden = 2\n",
    "size = 256\n",
    "m.add(layers.Dense(256, input_dim = x_train.shape[1], activation = activations)) # input layer requires input_dim param\n",
    "m.add(layers.Dense(256, activation = activations))\n",
    "m.add(layers.Dense(8, activation = activations))\n",
    "m.add(layers.Dense(8, activation='softmax'))\n",
    "m.summary()\n",
    "m.compile(loss=\"categorical_crossentropy\", optimizer= adam, metrics=['acc'])\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=-.01, patience=5, verbose=1, mode='auto')\n",
    "with tf.device('/cpu:0'):\n",
    "    m.fit(x_train, y_train, epochs = 100, shuffle = True, batch_size=128, verbose=1, callbacks=[es], validation_data=(x_test,y_test))\n",
    "    #score = m.evaluate(x_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33176119-17e4-416a-8a67-d3974c2994ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, best_model = optim.minimize(model=model, data=data, algo=tpe.suggest, max_evals=500, trials=Trials(), eval_space=True, notebook_name='Optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9854164-e338-4453-8774-47857b607e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f68c2-23f1-4c54-8d0b-96fa9c881fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c48d06-b9d5-48d9-a7b2-2f2d9650e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/281 [==============================] - 2s 6ms/step - loss: 0.4540 - categorical_hinge: 0.4271 - acc: 0.7720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4539780616760254, 0.4270787835121155, 0.7720490097999573]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c08db8-4f19-4b9d-8a6f-1ba8ca482813",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e26887e-3456-40c5-a42e-550342d07662",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions=np.argmax(predictions, axis=1)\n",
    "tests=np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38937d55-5338-444d-8bc8-d2daa542c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    5    0    7   19   23    0    0]\n",
      " [   0   40    0   67    0    0    0    0]\n",
      " [   1    0 1721    0   13   42    1    0]\n",
      " [   2  118    0   75    0    0    0    0]\n",
      " [  54    4    3    4  110  465    0    0]\n",
      " [ 223    0   71    0  752  774    4   14]\n",
      " [   2    0    7    0    1   13 2277   54]\n",
      " [   7    1    2    1    7   10   50 1935]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.02      0.01        55\n",
      "           1       0.24      0.37      0.29       107\n",
      "           2       0.95      0.97      0.96      1778\n",
      "           3       0.49      0.38      0.43       195\n",
      "           4       0.12      0.17      0.14       640\n",
      "           5       0.58      0.42      0.49      1838\n",
      "           6       0.98      0.97      0.97      2354\n",
      "           7       0.97      0.96      0.96      2013\n",
      "\n",
      "    accuracy                           0.77      8980\n",
      "   macro avg       0.54      0.53      0.53      8980\n",
      "weighted avg       0.80      0.77      0.78      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_predictions, tests))\n",
    "print(classification_report(y_predictions,tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc0008-6bb8-464f-979a-20ee64ae448e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
